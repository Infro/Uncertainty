using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.IO;
using System.Threading.Tasks;

using Microsoft.Research.Uncertain;
using Microsoft.Research.Uncertain.Inference;

using Lucene.Net.Analysis.Standard;
using Lucene.Net.Documents;
using Lucene.Net.Index;
using Lucene.Net.QueryParsers;
using Lucene.Net.Search;
using Lucene.Net.Store;
using Version = Lucene.Net.Util.Version;

namespace SearchEngine
{
    class Program
    {
        private static int number_of_machines = 3;
        // This is used by the  "central server" to prune the results of the other servers and return the final top-k.
        private static double threshold = 0.005;
        private static Dictionary<int, List<SampleData>> data_partitions_for_distributed_search = new Dictionary<int, List<SampleData>>();

        static Dictionary<int, List<SampleData>> CreateDataPartitions(List<SampleData> dataset, int number_of_machines)
        {
            Dictionary<int, List<SampleData>> partitions = new Dictionary<int, List<SampleData>>();
            Dictionary<int, int> partition_sizes = new Dictionary<int, int>();
            int partition_size = 0;

            // assuming all servers are equally powerful, we try to allocate the same number of searches to each of them. 
            partition_size = dataset.Count / number_of_machines;
            for (int x = 1; x <= number_of_machines; x++)
            {
                partition_sizes.Add(x, partition_size);
            }

            // if the dataset cannot be split equally, distribute the extras among the machines as evenly as possible.
            if (dataset.Count % number_of_machines != 0)
            {
                int extras = dataset.Count % number_of_machines;
                for (int x = 1; x <= extras; x++)
                {
                    partition_sizes[x] = partition_sizes[x] + 1;
                }
            }

            int index = 0;
            foreach (var machine_partition_size in partition_sizes)
            {
                List<SampleData> partition = new List<SampleData>();
                for (int x = index; x < index + machine_partition_size.Value; x++)
                {
                    partition.Add(dataset[x]);
                }
                index = index + machine_partition_size.Value;
                partitions.Add(machine_partition_size.Key, partition);               
            }
            return partitions;
        }

        public static void finalSearch(Dictionary<int, Dictionary<Field, double>> score_probabilities, Dictionary<int, Dictionary<Field, double>> score_summaries)
        {
            Console.Write("Central server's output: \n");
            foreach (var key in score_probabilities.Keys)
            {
                foreach (var key1 in score_probabilities[key].Keys)
                {
                    if (score_probabilities[key][key1] >= threshold)
                    {
                        Console.Write(key1 + " : " + score_probabilities[key][key1] + " : " + score_summaries[key][key1] + "\n");        
                    }
                               
                }
            }            
        }

        static void Main(string[] args)
        {
            StreamReader datafile = new StreamReader(@"C:\Users\t-chnand\Desktop\Uncertainty\InferenceSemantics\SearchEngine\SearchEngine\dataset\Data1.txt");
            DataParser.ParseDataSet(datafile);
            Dictionary<int, Dictionary<Field, double>> score_summaries = new Dictionary<int, Dictionary<Field, double>>();
            Dictionary<int, Dictionary<Field, double>> score_probabilities = new Dictionary<int, Dictionary<Field, double>>();
            //trying out inference
            //Uncertain<double> top_k = new Gaussian(5, 0.5);         
            //var list = top_k.SampledInference(50).Support();
            //int count = 1;
            //foreach (var l in list) 
            //{
              //  Console.Write(count +" : " + l + "\n");
              // count++; 
            //}
            try
            {
                data_partitions_for_distributed_search = CreateDataPartitions(SampleDataRepository.GetAll(), number_of_machines);
                int machine = 1;
                string query = "learning";
                // distribute search to available servers --- indexing and searching are both distributed. 
                foreach (var data_partition in data_partitions_for_distributed_search)
                {
                    // f(x) = lambda*e^(-lambda*x) is the pdf of exponential distribution. We model the probability of picking a document
                    // with a score x as an exponential distribution.
                    // MLE of lambda for exponential distribution is the reciprocal of sample mean, where the sample is the reciprocals of the normalized scores generated by the servers.
                    // smaller the value of the reciprocal, the larger the probability of picking it since the score is larger.
                    double lambda_mle = 0.0;
                    HashSet<double> unique_normalized_score_reciprocals = new HashSet<double>();
                    string score_file = "scores" + machine.ToString() + ".txt";

                    using (StreamWriter sw = new StreamWriter(score_file))
                    {
                        Dictionary<Field, double> normalized_scores = new Dictionary<Field, double>();
                        Dictionary<Field, double> document_probabilities = new Dictionary<Field, double>();
                        Console.Write("\nMachine " + machine + " building indexes\n");
                        Index indexer = new Index();
                        indexer.rebuildIndex(data_partition.Value);
                        Console.Write("Building indexes done\n");
                        Console.Write("Machine " + machine + " performing search\n");
                        Search s = new Search();
                        TopDocs topDocs = s.performSearch(query, data_partition.Value.Count);                       
                        Console.Write("Results found: " + topDocs.TotalHits + "\n");
                        ScoreDoc[] hits = topDocs.ScoreDocs;
                        double sum_of_score_reciprocals = 0.0;
                        for (int x = 0; x < hits.Length; x++)
                        {
                            Document doc = s.getDocument(hits[x].Doc);
                            double normalized_score = hits[x].Score / topDocs.MaxScore;
                            double normalized_score_reciprocal = topDocs.MaxScore / hits[x].Score;
                            unique_normalized_score_reciprocals.Add(normalized_score_reciprocal);
                            sum_of_score_reciprocals = sum_of_score_reciprocals + normalized_score_reciprocal;
                            Console.Write(doc.GetField("Id") + " " + doc.GetField("Original title") + " " + doc.GetField("Normalized title") + " " + hits[x].Score);
                            Console.Write("\n");
                            normalized_scores.Add(doc.GetField("Id"), normalized_score);
                            sw.Write(normalized_score);
                            sw.Write(Environment.NewLine);
                        }

                        lambda_mle = unique_normalized_score_reciprocals.Count / sum_of_score_reciprocals;
                        
                        // probability associated with picking a document with a reciprocal score S is then lambda.e^(-lambda.S)                        
                        foreach (var key in normalized_scores.Keys)
                        {                            
                            document_probabilities.Add(key, lambda_mle*Math.Exp(-lambda_mle*(1/normalized_scores[key])));
                        }                        
                        score_summaries.Add(machine, normalized_scores);
                        score_probabilities.Add(machine, document_probabilities);
                        machine++;
                        Console.Write("Finished\n");
                    }
                }
                // final search in the "central server" using results from the other servers
                finalSearch(score_probabilities, score_summaries);
            }
            catch (Exception e)
            {
                Console.Write("Search failed: " + e.GetType());
            }
            Console.ReadKey();
        }
    }
}
